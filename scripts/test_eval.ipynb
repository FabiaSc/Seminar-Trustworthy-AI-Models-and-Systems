{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45634380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-fid in /home/group-3/miniconda3/lib/python3.13/site-packages (0.1.35)\n",
      "Requirement already satisfied: torch in /home/group-3/miniconda3/lib/python3.13/site-packages (from clean-fid) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /home/group-3/miniconda3/lib/python3.13/site-packages (from clean-fid) (0.21.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.14.3 in /home/group-3/miniconda3/lib/python3.13/site-packages (from clean-fid) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /home/group-3/miniconda3/lib/python3.13/site-packages (from clean-fid) (1.16.3)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /home/group-3/miniconda3/lib/python3.13/site-packages (from clean-fid) (4.67.1)\n",
      "Requirement already satisfied: pillow>=8.1 in /home/group-3/miniconda3/lib/python3.13/site-packages (from clean-fid) (11.3.0)\n",
      "Requirement already satisfied: requests in /home/group-3/miniconda3/lib/python3.13/site-packages (from clean-fid) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/group-3/miniconda3/lib/python3.13/site-packages (from requests->clean-fid) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/group-3/miniconda3/lib/python3.13/site-packages (from requests->clean-fid) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/group-3/miniconda3/lib/python3.13/site-packages (from requests->clean-fid) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/group-3/miniconda3/lib/python3.13/site-packages (from requests->clean-fid) (2025.10.5)\n",
      "Requirement already satisfied: filelock in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/group-3/miniconda3/lib/python3.13/site-packages (from torch->clean-fid) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/group-3/miniconda3/lib/python3.13/site-packages (from sympy==1.13.1->torch->clean-fid) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/group-3/miniconda3/lib/python3.13/site-packages (from jinja2->torch->clean-fid) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install clean-fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1add59bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /home/group-3/miniconda3/bin/python\n",
      "Host: lyskamm-1-1xl40s-4 | OS: Linux-6.8.0-86-generic-x86_64-with-glibc2.39\n",
      "CWD: /home/group-3/tests_L/MixDQ/scripts\n"
     ]
    }
   ],
   "source": [
    "import os, sys, socket, platform\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Host:\", socket.gethostname(), \"| OS:\", platform.platform())\n",
    "print(\"CWD:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8f35c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/group-3/tests_L/MixDQ/scripts/utils/val2014: 40504 images\n",
      "sample: ['/home/group-3/tests_L/MixDQ/scripts/utils/val2014/COCO_val2014_000000407960.jpg', '/home/group-3/tests_L/MixDQ/scripts/utils/val2014/COCO_val2014_000000577868.jpg', '/home/group-3/tests_L/MixDQ/scripts/utils/val2014/COCO_val2014_000000573395.jpg']\n",
      "/home/group-3/tests_L/MixDQ/logs/sdxl_fp_eval_big/generated_images: 1024 images\n",
      "sample: ['/home/group-3/tests_L/MixDQ/logs/sdxl_fp_eval_big/generated_images/878.png', '/home/group-3/tests_L/MixDQ/logs/sdxl_fp_eval_big/generated_images/349.png', '/home/group-3/tests_L/MixDQ/logs/sdxl_fp_eval_big/generated_images/268.png']\n",
      "/home/group-3/tests_L/MixDQ/logs/sdxl_mixdq_eval_images_big: 1024 images\n",
      "sample: ['/home/group-3/tests_L/MixDQ/logs/sdxl_mixdq_eval_images_big/878.png', '/home/group-3/tests_L/MixDQ/logs/sdxl_mixdq_eval_images_big/349.png', '/home/group-3/tests_L/MixDQ/logs/sdxl_mixdq_eval_images_big/268.png']\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "def count_recursive(root):\n",
    "    root = os.path.abspath(os.path.expanduser(root))\n",
    "    exts = ('.png','.jpg','.jpeg','.bmp','.webp')\n",
    "    files = [p for p in glob.iglob(os.path.join(root, '**', '*'), recursive=True)\n",
    "             if p.lower().endswith(exts)]\n",
    "    print(f\"{root}: {len(files)} images\")\n",
    "    print(\"sample:\", files[:3])\n",
    "    return root, files\n",
    "\n",
    "# CHANGE REAL to your actual COCO val2014 images folder (not the repo's scripts/utils/)\n",
    "REAL = \"/home/group-3/tests_L/MixDQ/scripts/utils/val2014\"  # e.g. /data/coco/val2014\n",
    "FP16 = \"/home/group-3/tests_L/MixDQ/logs/sdxl_fp_eval_big/generated_images\"\n",
    "MIXD = \"/home/group-3/tests_L/MixDQ/logs/sdxl_mixdq_eval_images_big\"\n",
    "\n",
    "real_root, real_list = count_recursive(REAL)\n",
    "fp_root,   fp_list   = count_recursive(FP16)\n",
    "mix_root,  mix_list  = count_recursive(MIXD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat dirs: /tmp/fid_flat_real /tmp/fid_flat_fp16 /tmp/fid_flat_mixd\n",
      "compute FID between two folders\n",
      "Found 40504 images in the folder /tmp/fid_flat_real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID fid_flat_real : 100%|██████████| 1266/1266 [07:46<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1024 images in the folder /tmp/fid_flat_fp16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID fid_flat_fp16 : 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n",
      "Found 40504 images in the folder /tmp/fid_flat_real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID fid_flat_real : 100%|██████████| 1266/1266 [09:00<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images in the folder /tmp/fid_flat_mixd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID fid_flat_mixd : 100%|██████████| 2/2 [00:00<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID FP16: 85.81413421718582  | FID MixDQ: 232.16568439699995\n",
      "Found 40504 images in the folder /tmp/fid_flat_real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "custom stats: fid_flat_real : 100%|██████████| 633/633 [06:26<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving custom FID stats to /home/group-3/miniconda3/lib/python3.13/site-packages/cleanfid/stats/coco2014_val_flat_clean_custom_na.npz\n",
      "saving custom KID stats to /home/group-3/miniconda3/lib/python3.13/site-packages/cleanfid/stats/coco2014_val_flat_clean_custom_na_kid.npz\n",
      "compute FID of a folder with coco2014_val_flat statistics\n",
      "Found 1024 images in the folder /tmp/fid_flat_fp16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID fid_flat_fp16 : 100%|██████████| 32/32 [00:25<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID of a folder with coco2014_val_flat statistics\n",
      "Found 64 images in the folder /tmp/fid_flat_mixd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID fid_flat_mixd : 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID (custom stats) FP16: 85.81415229915007  | MixDQ: 232.16348995764372\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil\n",
    "from cleanfid import fid\n",
    "\n",
    "def flatten_symlink(files, out_dir):\n",
    "    out = pathlib.Path(out_dir).resolve()\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    for i, src in enumerate(files):\n",
    "        ext = pathlib.Path(src).suffix.lower() or \".png\"\n",
    "        dst = out / f\"{i:08d}{ext}\"\n",
    "        try:\n",
    "            os.symlink(os.path.abspath(src), dst)   # fast, no copy\n",
    "        except (OSError, AttributeError):           # fallback if symlinks not allowed\n",
    "            shutil.copy2(src, dst)\n",
    "    return str(out)\n",
    "\n",
    "flat_real = flatten_symlink(real_list, \"/tmp/fid_flat_real\")\n",
    "flat_fp16 = flatten_symlink(fp_list,   \"/tmp/fid_flat_fp16\")\n",
    "flat_mixd = flatten_symlink(mix_list,  \"/tmp/fid_flat_mixd\")\n",
    "\n",
    "print(\"Flat dirs:\", flat_real, flat_fp16, flat_mixd)\n",
    "\n",
    "# Option A: folder vs folder\n",
    "fid_fp16  = fid.compute_fid(flat_real, flat_fp16, mode=\"clean\", num_workers=0, verbose=True)\n",
    "fid_mixdq = fid.compute_fid(flat_real, flat_mixd, mode=\"clean\", num_workers=0, verbose=True)\n",
    "print(\"FID FP16:\", fid_fp16, \" | FID MixDQ:\", fid_mixdq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea7d2480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/group-3/tests_L/MixDQ/scripts/utils/prompts.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "COCO_JSON = \"/home/group-3/tests_L/MixDQ/scripts/utils/captions_val2014.json\"\n",
    "OUT_TXT   = \"/home/group-3/tests_L/MixDQ/scripts/utils/prompts.txt\"\n",
    "\n",
    "with open(COCO_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# map image_id -> first caption\n",
    "caps_by_id = {}\n",
    "for ann in coco[\"annotations\"]:\n",
    "    caps_by_id.setdefault(ann[\"image_id\"], ann[\"caption\"].strip())\n",
    "\n",
    "# order images by file_name (lexicographic), then take the first caption for each\n",
    "images_sorted = sorted(coco[\"images\"], key=lambda x: x[\"file_name\"])\n",
    "prompts = [caps_by_id.get(img[\"id\"], \"a photo\") for img in images_sorted]\n",
    "\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(prompts))\n",
    "print(\"Wrote:\", OUT_TXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca1b906",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'open_clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Torch / CLIP\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Clean-FID\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'open_clip'"
     ]
    }
   ],
   "source": [
    "# === ONE-CELL: FID + CLIPScore + RI@1 =========================================\n",
    "# Fill these with ABSOLUTE paths on your VM:\n",
    "REAL_DIR   = \"/home/group-3/tests_L/MixDQ/scripts/utils/val2014\"                    # real images (COCO val2014, or your reference set)\n",
    "FP16_DIR   = \"/home/group-3/tests_L/MixDQ/logs/sdxl_fp_eval_big/generated_images\"\n",
    "MIXDQ_DIR  = \"/home/group-3/tests_L/MixDQ/logs/sdxl_mixdq_eval_images_big\"\n",
    "PROMPTS_TXT = \"/home/group-3/tests_L/MixDQ/scripts/utils/prompts.txt\"                      # one prompt per image, same order as the generated images\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "import os, sys, glob, math, tempfile, shutil, pathlib\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Torch / CLIP\n",
    "import torch, torch.nn.functional as F\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Clean-FID\n",
    "from cleanfid import fid\n",
    "\n",
    "# ---- helpers -----------------------------------------------------------------\n",
    "def _abs(p): return os.path.abspath(os.path.expanduser(p))\n",
    "\n",
    "def list_images_recursive(root: str, exts=(\".png\",\".jpg\",\".jpeg\",\".bmp\",\".webp\")) -> List[str]:\n",
    "    root = _abs(root)\n",
    "    return [p for p in glob.iglob(os.path.join(root, \"**\", \"*\"), recursive=True)\n",
    "            if os.path.isfile(p) and p.lower().endswith(exts)]\n",
    "\n",
    "def flatten_to_tmp(files: List[str], tmp_label: str) -> str:\n",
    "    out = pathlib.Path(tempfile.mkdtemp(prefix=f\"fid_flat_{tmp_label}_\")).resolve()\n",
    "    for i, src in enumerate(files):\n",
    "        ext = pathlib.Path(src).suffix.lower() or \".png\"\n",
    "        dst = out / f\"{i:08d}{ext}\"\n",
    "        try:\n",
    "            os.symlink(_abs(src), dst)  # fast if fs allows\n",
    "        except Exception:\n",
    "            shutil.copy2(_abs(src), dst)\n",
    "    return str(out)\n",
    "\n",
    "def read_prompts(prompts_path: str) -> List[str]:\n",
    "    with open(_abs(prompts_path), \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "    if not lines:\n",
    "        raise ValueError(\"No prompts found in PROMPTS_TXT.\")\n",
    "    return lines\n",
    "\n",
    "def natural_key(s: str):\n",
    "    import re\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r\"(\\d+)\", s)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_images(files: List[str], model, preprocess, device, batch=64) -> torch.Tensor:\n",
    "    feats = []\n",
    "    for i in tqdm(range(0, len(files), batch), desc=\"embed images\"):\n",
    "        ims = []\n",
    "        for p in files[i:i+batch]:\n",
    "            try:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                continue\n",
    "            ims.append(preprocess(img))\n",
    "        if not ims:\n",
    "            continue\n",
    "        ims = torch.stack(ims).to(device)\n",
    "        f = model.encode_image(ims)\n",
    "        feats.append(F.normalize(f.float(), dim=-1))\n",
    "    if not feats:\n",
    "        raise ValueError(\"No image features computed (check image readability).\")\n",
    "    return torch.cat(feats, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(texts: List[str], model, tokenizer, device, batch=256) -> torch.Tensor:\n",
    "    feats = []\n",
    "    for i in tqdm(range(0, len(texts), batch), desc=\"embed texts\"):\n",
    "        toks = tokenizer(texts[i:i+batch]).to(device)\n",
    "        f = model.encode_text(toks)\n",
    "        feats.append(F.normalize(f.float(), dim=-1))\n",
    "    return torch.cat(feats, dim=0)\n",
    "\n",
    "def clipscore_and_ri1(gen_files: List[str], prompts: List[str], device=\"cuda\") -> Tuple[float, float]:\n",
    "    # Load CLIP\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-L-14-336\", pretrained=\"openai\", device=device)\n",
    "    tokenizer = open_clip.get_tokenizer(\"ViT-L-14-336\")\n",
    "    model.eval()\n",
    "\n",
    "    # Sort for deterministic alignment\n",
    "    files_sorted = sorted(gen_files, key=natural_key)\n",
    "\n",
    "    if len(files_sorted) != len(prompts):\n",
    "        raise ValueError(f\"Mismatch: {len(files_sorted)} images vs {len(prompts)} prompts. \"\n",
    "                         f\"Ensure PROMPTS_TXT lines match generated images 1:1 (same order as sorted filenames).\")\n",
    "\n",
    "    img_feat = embed_images(files_sorted, model, preprocess, device)\n",
    "    txt_feat = embed_texts(prompts, model, tokenizer, device)\n",
    "\n",
    "    # Similarity matrix (text x image), cosine since features are normalized\n",
    "    S = txt_feat @ img_feat.T\n",
    "    diag = torch.diag(S)\n",
    "    clipscore_mean = (torch.clamp(diag, min=0.0) * 2.5).mean().item()  # standard scaling\n",
    "\n",
    "    # RI@1: does each prompt retrieve its own image as top-1?\n",
    "    top1 = S.argmax(dim=1)\n",
    "    ri1 = (top1 == torch.arange(S.size(0), device=top1.device)).float().mean().item()\n",
    "\n",
    "    return clipscore_mean, ri1\n",
    "\n",
    "def compute_fid_folder_vs_folder(real_files: List[str], gen_files: List[str], tag: str) -> float:\n",
    "    flat_real = flatten_to_tmp(real_files, f\"real_{tag}\")\n",
    "    flat_gen  = flatten_to_tmp(gen_files,  f\"gen_{tag}\")\n",
    "    # Folder-vs-folder FID (non-recursive; we pre-flatten via symlink/copy)\n",
    "    score = fid.compute_fid(flat_real, flat_gen, mode=\"clean\", num_workers=0, verbose=True)\n",
    "    # Cleanup note: temp dirs auto-cleaned on reboot; keep for reproducibility this run.\n",
    "    print(f\"[{tag}] temp real dir:\", flat_real)\n",
    "    print(f\"[{tag}] temp gen  dir:\", flat_gen)\n",
    "    return score\n",
    "\n",
    "# ---- run ---------------------------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# 1) Collect files (recursively), and sort deterministically\n",
    "real_files = sorted(list_images_recursive(REAL_DIR), key=natural_key)\n",
    "fp16_files = sorted(list_images_recursive(FP16_DIR), key=natural_key)\n",
    "mixd_files = sorted(list_images_recursive(MIXDQ_DIR), key=natural_key)\n",
    "\n",
    "print(f\"Found: real={len(real_files)} | fp16={len(fp16_files)} | mixdq={len(mixd_files)}\")\n",
    "if len(real_files) == 0 or len(fp16_files) == 0 or len(mixd_files) == 0:\n",
    "    raise FileNotFoundError(\"One or more folders have 0 readable images. Check the ABSOLUTE paths and extensions.\")\n",
    "\n",
    "# 2) Load prompts (one per image, in the SAME order as the sorted filenames above)\n",
    "prompts = read_prompts(PROMPTS_TXT)\n",
    "\n",
    "# 3) CLIPScore & RI@1\n",
    "clip_fp16, ri1_fp16 = clipscore_and_ri1(fp16_files, prompts, device=device)\n",
    "clip_mixd, ri1_mixd = clipscore_and_ri1(mixd_files, prompts, device=device)\n",
    "\n",
    "# 4) FID (folder vs folder; Clean-FID \"clean\" mode)\n",
    "fid_fp16  = compute_fid_folder_vs_folder(real_files, fp16_files, tag=\"FP16\")\n",
    "fid_mixdq = compute_fid_folder_vs_folder(real_files, mixd_files,  tag=\"MixDQ\")\n",
    "\n",
    "# 5) Print summary\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"CLIPScore (mean)  FP16:  {clip_fp16:.4f}    MixDQ: {clip_mixd:.4f}\")\n",
    "print(f\"RI@1              FP16:  {ri1_fp16*100:5.2f}%  MixDQ: {ri1_mixd*100:5.2f}%\")\n",
    "print(f\"FID (clean)       FP16:  {fid_fp16:.4f}       MixDQ: {fid_mixdq:.4f}\")\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89cb4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Found files: real=40504 | fp16=1024 | mixdq=1024\n",
      "CLIP backend: hf_clip\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch: 1024 images vs 40504 prompts.\nEnsure prompts.txt lines match the sorted filenames 1:1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    176\u001b[39m backend = ClipBackend(device=device)\n\u001b[32m    177\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCLIP backend:\u001b[39m\u001b[33m\"\u001b[39m, backend.name)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m clip_fp16, ri1_fp16 = \u001b[43mclipscore_and_ri1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp16_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m clip_mixd, ri1_mixd = clipscore_and_ri1(mixd_files, prompts, backend)\n\u001b[32m    182\u001b[39m fid_fp16  = compute_fid_folder_vs_folder(real_files, fp16_files, tag=\u001b[33m\"\u001b[39m\u001b[33mFP16\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mclipscore_and_ri1\u001b[39m\u001b[34m(gen_files, prompts, backend)\u001b[39m\n\u001b[32m    140\u001b[39m files_sorted = \u001b[38;5;28msorted\u001b[39m(gen_files, key=natural_key)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files_sorted) != \u001b[38;5;28mlen\u001b[39m(prompts):\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMismatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files_sorted)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prompts.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    143\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnsure prompts.txt lines match the sorted filenames 1:1.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m img_feat = backend.embed_images(files_sorted)\n\u001b[32m    145\u001b[39m txt_feat = backend.embed_texts(prompts)\n",
      "\u001b[31mValueError\u001b[39m: Mismatch: 1024 images vs 40504 prompts.\nEnsure prompts.txt lines match the sorted filenames 1:1."
     ]
    }
   ],
   "source": [
    "# === ONE-CELL: FID + CLIPScore + RI@1 with open_clip OR HF fallback ===========\n",
    "# >>>>>> EDIT THESE (absolute paths) <<<<<<\n",
    "REAL_DIR   = \"/home/group-3/tests_L/MixDQ/scripts/utils/val2014\"                    # real images (COCO val2014, or your reference set)\n",
    "FP16_DIR   = \"/home/group-3/tests_L/MixDQ/logs/sdxl_fp_eval_big/generated_images\"\n",
    "MIXDQ_DIR  = \"/home/group-3/tests_L/MixDQ/logs/sdxl_mixdq_eval_images_big\"\n",
    "PROMPTS_TXT = \"/home/group-3/tests_L/MixDQ/scripts/utils/prompts.txt\"                       # one prompt per image, same order as filenames\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "import os, sys, glob, tempfile, shutil, pathlib, math, types\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Clean-FID\n",
    "try:\n",
    "    from cleanfid import fid\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"clean-fid is required. Install with: pip install clean-fid\") from e\n",
    "\n",
    "def _abs(p): return os.path.abspath(os.path.expanduser(p))\n",
    "\n",
    "def list_images_recursive(root: str, exts=(\".png\",\".jpg\",\".jpeg\",\".bmp\",\".webp\")) -> List[str]:\n",
    "    root = _abs(root)\n",
    "    return [p for p in glob.iglob(os.path.join(root, \"**\", \"*\"), recursive=True)\n",
    "            if os.path.isfile(p) and p.lower().endswith(exts)]\n",
    "\n",
    "def flatten_to_tmp(files: List[str], tmp_label: str) -> str:\n",
    "    out = pathlib.Path(tempfile.mkdtemp(prefix=f\"fid_flat_{tmp_label}_\")).resolve()\n",
    "    for i, src in enumerate(files):\n",
    "        ext = pathlib.Path(src).suffix.lower() or \".png\"\n",
    "        dst = out / f\"{i:08d}{ext}\"\n",
    "        try:\n",
    "            os.symlink(_abs(src), dst)    # fast, if allowed\n",
    "        except Exception:\n",
    "            shutil.copy2(_abs(src), dst)  # fallback: copy\n",
    "    return str(out)\n",
    "\n",
    "def read_prompts(prompts_path: str) -> List[str]:\n",
    "    with open(_abs(prompts_path), \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "    if not lines:\n",
    "        raise ValueError(\"No prompts found in PROMPTS_TXT.\")\n",
    "    return lines\n",
    "\n",
    "def natural_key(s: str):\n",
    "    import re\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r\"(\\d+)\", os.path.basename(s))]\n",
    "\n",
    "# ---- CLIP backends ------------------------------------------------------------\n",
    "class ClipBackend:\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.name = None\n",
    "        self.model = None\n",
    "        self.preprocess = None\n",
    "        self.tokenizer = None\n",
    "        self.processor = None\n",
    "\n",
    "        # Try open_clip\n",
    "        try:\n",
    "            import open_clip\n",
    "            self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
    "                \"ViT-L-14-336\", pretrained=\"openai\", device=device\n",
    "            )\n",
    "            self.tokenizer = open_clip.get_tokenizer(\"ViT-L-14-336\")\n",
    "            self.model.eval()\n",
    "            self.name = \"open_clip\"\n",
    "        except Exception:\n",
    "            # Fallback: Hugging Face transformers\n",
    "            try:\n",
    "                from transformers import CLIPModel, CLIPProcessor\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    \"Neither open_clip_torch nor transformers (CLIP) is available.\\n\"\n",
    "                    \"Install one of:\\n\"\n",
    "                    \"  pip install open-clip-torch\\n\"\n",
    "                    \"  or\\n\"\n",
    "                    \"  pip install transformers\"\n",
    "                ) from e\n",
    "            from transformers import CLIPModel, CLIPProcessor\n",
    "            self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
    "            self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "            self.model.eval()\n",
    "            self.name = \"hf_clip\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_images(self, files: List[str], batch: int = 64) -> torch.Tensor:\n",
    "        feats = []\n",
    "        if self.name == \"open_clip\":\n",
    "            for i in tqdm(range(0, len(files), batch), desc=\"embed images (open_clip)\"):\n",
    "                imgs = []\n",
    "                for p in files[i:i+batch]:\n",
    "                    try:\n",
    "                        imgs.append(self.preprocess(Image.open(p).convert(\"RGB\")))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if not imgs: \n",
    "                    continue\n",
    "                ims = torch.stack(imgs).to(self.device)\n",
    "                f = self.model.encode_image(ims)\n",
    "                feats.append(F.normalize(f.float(), dim=-1))\n",
    "        else:  # hf_clip\n",
    "            for i in tqdm(range(0, len(files), batch), desc=\"embed images (hf_clip)\"):\n",
    "                imgs = []\n",
    "                for p in files[i:i+batch]:\n",
    "                    try:\n",
    "                        imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if not imgs:\n",
    "                    continue\n",
    "                inputs = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
    "                f = self.model.get_image_features(**inputs)\n",
    "                feats.append(F.normalize(f.float(), dim=-1))\n",
    "        if not feats:\n",
    "            raise ValueError(\"No image features computed (check image readability).\")\n",
    "        return torch.cat(feats, dim=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_texts(self, texts: List[str], batch: int = 256) -> torch.Tensor:\n",
    "        feats = []\n",
    "        if self.name == \"open_clip\":\n",
    "            tok = self.tokenizer\n",
    "            for i in tqdm(range(0, len(texts), batch), desc=\"embed texts (open_clip)\"):\n",
    "                t = tok(texts[i:i+batch]).to(self.device)\n",
    "                f = self.model.encode_text(t)\n",
    "                feats.append(F.normalize(f.float(), dim=-1))\n",
    "        else:  # hf_clip\n",
    "            for i in tqdm(range(0, len(texts), batch), desc=\"embed texts (hf_clip)\"):\n",
    "                inputs = self.processor(text=texts[i:i+batch], return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "                f = self.model.get_text_features(**inputs)\n",
    "                feats.append(F.normalize(f.float(), dim=-1))\n",
    "        return torch.cat(feats, dim=0)\n",
    "\n",
    "# ---- Metrics ------------------------------------------------------------------\n",
    "def clipscore_and_ri1(gen_files: List[str], prompts: List[str], backend: ClipBackend) -> Tuple[float, float]:\n",
    "    files_sorted = sorted(gen_files, key=natural_key)\n",
    "    if len(files_sorted) != len(prompts):\n",
    "        raise ValueError(f\"Mismatch: {len(files_sorted)} images vs {len(prompts)} prompts.\\n\"\n",
    "                         f\"Ensure prompts.txt lines match the sorted filenames 1:1.\")\n",
    "    img_feat = backend.embed_images(files_sorted)\n",
    "    txt_feat = backend.embed_texts(prompts)\n",
    "\n",
    "    S = txt_feat @ img_feat.T             # cosine since features are normalized\n",
    "    diag = torch.diag(S)\n",
    "    clipscore_mean = (torch.clamp(diag, min=0.0) * 2.5).mean().item()  # standard scaling\n",
    "    top1 = S.argmax(dim=1)\n",
    "    ri1 = (top1 == torch.arange(S.size(0), device=top1.device)).float().mean().item()\n",
    "    return clipscore_mean, ri1\n",
    "\n",
    "def compute_fid_folder_vs_folder(real_files: List[str], gen_files: List[str], tag: str) -> float:\n",
    "    flat_real = flatten_to_tmp(real_files, f\"real_{tag}\")\n",
    "    flat_gen  = flatten_to_tmp(gen_files,  f\"gen_{tag}\")\n",
    "    score = fid.compute_fid(flat_real, flat_gen, mode=\"clean\", num_workers=0, verbose=True)\n",
    "    print(f\"[{tag}] temp real dir:\", flat_real)\n",
    "    print(f\"[{tag}] temp gen  dir:\", flat_gen)\n",
    "    return score\n",
    "\n",
    "# ---- Run ----------------------------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "real_files = sorted(list_images_recursive(REAL_DIR), key=natural_key)\n",
    "fp16_files = sorted(list_images_recursive(FP16_DIR), key=natural_key)\n",
    "mixd_files = sorted(list_images_recursive(MIXDQ_DIR), key=natural_key)\n",
    "\n",
    "print(f\"Found files: real={len(real_files)} | fp16={len(fp16_files)} | mixdq={len(mixd_files)}\")\n",
    "if len(real_files) == 0 or len(fp16_files) == 0 or len(mixd_files) == 0:\n",
    "    raise FileNotFoundError(\"One or more folders have 0 readable images. Use absolute paths and ensure top-level images or nested (we handle nested).\")\n",
    "\n",
    "prompts = read_prompts(PROMPTS_TXT)\n",
    "\n",
    "backend = ClipBackend(device=device)\n",
    "print(\"CLIP backend:\", backend.name)\n",
    "\n",
    "clip_fp16, ri1_fp16 = clipscore_and_ri1(fp16_files, prompts, backend)\n",
    "clip_mixd, ri1_mixd = clipscore_and_ri1(mixd_files, prompts, backend)\n",
    "\n",
    "fid_fp16  = compute_fid_folder_vs_folder(real_files, fp16_files, tag=\"FP16\")\n",
    "fid_mixdq = compute_fid_folder_vs_folder(real_files, mixd_files,  tag=\"MixDQ\")\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"CLIPScore (mean)  FP16:  {clip_fp16:.4f}    MixDQ: {clip_mixd:.4f}\")\n",
    "print(f\"RI@1              FP16:  {ri1_fp16*100:5.2f}%  MixDQ: {ri1_mixd*100:5.2f}%\")\n",
    "print(f\"FID (clean)       FP16:  {fid_fp16:.4f}       MixDQ: {fid_mixdq:.4f}\")\n",
    "# ==============================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
